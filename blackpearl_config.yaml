# This file can update the JupyterHub Helm chart's default configuration values.
#
# For reference see the configuration reference and default values, but make
# sure to refer to the Helm chart version of interest to you!
#
# Introduction to YAML:     https://www.youtube.com/watch?v=cdLNKUoMc6c
# Chart config reference:   https://zero-to-jupyterhub.readthedocs.io/en/stable/resources/reference.html
# Chart default values:     https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/HEAD/jupyterhub/values.yaml
# Available chart versions: https://jupyterhub.github.io/helm-chart/

# disable user and server culling
cull:
  enabled: false

# from https://z2jh.jupyter.org/en/stable/administrator/optimization.html
# put users on the same node first (to leave larger groups of GPUs avaliable)
scheduling:
  userScheduler:
    enabled: true

# Update images here
# NOTE: Images need to be updated in a second place in this file.
#       This only updates the image puller, so that new images get
#       pulled during the helm upgrade.
prePuller:
  extraImages:
    datascience:
      name: brineylab/jupyterhub-datascience
      tag: v2024-09-12
    deeplearning:
      name: brineylab/jupyterhub-deeplearning
      tag: v2024-09-12

singleuser:
  cmd: jupyterhub-singleuser
  defaultUrl: lab

  # networking (notebook server spawning fails if networkPolicy is set to true)
  networkPolicy:
    enabled: false

  # set extra environmental vars
  extraEnv:
    GRANT_SUDO: "yes"
  uid: 0
  fsGid: 0

  # defaults - in case pods get launched w/o specifying profile
  # i think this can only happen from the admin page
  memory:
    limit: 16G
    guarantee: 16G
  cpu:
    limit: 16
    guarantee: 16
  extraResource:
    limits:
      ephemeral-storage: "750Gi"
    guarantees:
      ephemeral-storage: "20Gi"
  image:
    name: "brineylab/jupyterhub-datascience"
    tag: "v2024-09-12"

  # required for codeserver to launch
  lifecycleHooks:
    postStart:
      exec:
        command:
          - "sh"
          - "-c"
          - >
            mkdir -p ${HOME}/codeserver;
            mkdir -p ${HOME}/.local;
            chown -R ${NB_UID:-1000}:${NB_GID:-100} ${HOME}/codeserver;
            chown -R ${NB_UID:-1000}:${NB_GID:-100} ${HOME}/.local;

  storage:
    dynamic:
      storageClass: nfs-sparrow-jh-nvme
    capacity: 250Gi

    extraVolumes:
      # shared
      - name: nfs-davyjones-shared
        persistentVolumeClaim:
          claimName: nfs-davyjones-shared
      # references
      - name: nfs-wallace-references
        persistentVolumeClaim:
          claimName: nfs-wallace-references
          readOnly: true
      # workspace (sparrow)
      - name: nfs-sparrow-workspace
        persistentVolumeClaim:
          claimName: nfs-sparrow-workspace
      # sequencing data: NovaSeq
      - name: nfs-avon-novaseq6k
        persistentVolumeClaim:
          claimName: nfs-avon-novaseq6k
      # sequencing data: NextSeq
      - name: nfs-avon-nextseq2k
        persistentVolumeClaim:
          claimName: nfs-avon-nextseq2k
      # sequencing data: ONT
      - name: nfs-stringer-ont
        persistentVolumeClaim:
          claimName: nfs-stringer-ont
      # dshm - needed for training on gpu servers running jupyterhub
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: "10Gi"
    extraVolumeMounts:
      - name: nfs-davyjones-shared
        mountPath: /home/jovyan/shared
      - name: nfs-wallace-references
        mountPath: /home/jovyan/references
      - name: nfs-sparrow-workspace
        mountPath: /home/jovyan/workspace
      - name: nfs-avon-novaseq6k
        mountPath: /home/jovyan/sequencing_runs/novaseq
      - name: nfs-avon-nextseq2k
        mountPath: /home/jovyan/sequencing_runs/nextseq
      - name: nfs-stringer-ont
        mountPath: /home/jovyan/sequencing_runs/ont
      - name: dshm
        mountPath: /dev/shm

hub:
  networkPolicy:
    enabled: false

  containerSecurityContext:
    runAsUser: 0
    runAsGroup: 0

  # mount users and home page configmaps
  extraVolumes:
    - name: templates
      configMap:
        name: templates
    - name: extra-config
      configMap:
        name: extra-config
  extraVolumeMounts:
    - mountPath: /etc/jupyterhub/templates
      name: templates
    - mountPath: /etc/jupyterhub/config
      name: extra-config

  config:
    JupyterHub:
      authenticator_class: "nativeauthenticator.NativeAuthenticator"
      default_url: "/hub/home"

    NativeAuthenticator:
      enable_signup: true

  extraConfig:
    # set env variables
    env: |
      c.KubeSpawner.environment = {'NB_UMASK': '0000'}
      c.KubeSpawner.allow_privilege_escalation = True
      c.KubeSpawner.args = ["--allow-root"]

    # add custom templates to template path
    templates: |
      import os, nativeauthenticator

      # inject custom templates
      c.JupyterHub.template_paths = [f"{os.path.dirname(nativeauthenticator.__file__)}/templates/"]
      c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")

      # set url for gpu status
      c.JupyterHub.template_vars.update({
          "gpu_status_url": "http://172.29.80.65:5000/gpu_status_bynode"
      })

    # set admin and allowed users -> allow admin and gpu users only
    users: |
      import yaml
      with open("/etc/jupyterhub/config/users.yaml") as f:
        users = yaml.safe_load(f)

      c.Authenticator.admin_users = users.get("admin", [])
      c.Authenticator.allowed_users = set(users.get("admin", []) + users.get("gpu", []))

    # handle custom named server limit
    # ref: https://jupyterhub.readthedocs.io/en/stable/howto/configuration/config-user-env.html#named-servers
    # src code: https://github.com/jupyterhub/jupyterhub/blob/main/jupyterhub/app.py#L1341
    namedServers: |
      def custom_named_server_limit(handler):
        with open("/etc/jupyterhub/config/users.yaml") as f:
          users = yaml.safe_load(f)

        user = handler.current_user.name

        if user in users.get("admin", []):
            return 0 # unlimited

        return 1 # 2 servers total

      c.JupyterHub.allow_named_servers = True
      c.JupyterHub.named_server_limit_per_user = custom_named_server_limit

    # custom profiles
    # import options form and pre-spawn hook helper fns from extra-config configmap
    options-form: |
      import yaml
      import sys

      # import functions
      sys.path.append('/etc/jupyterhub/config')
      from options_form import dynamic_options_form_withconfig
      from prespawn_hook import set_spawner_resources

      # read node info
      with open("/etc/jupyterhub/config/nodes.yaml", "r") as file:
        node_info = yaml.safe_load(file)

      # update images and options for number of GPUs here
      CONFIG = {
        "images": {
            "datascience": "brineylab/jupyterhub-datascience:v2024-09-12",
            "deeplearning": "brineylab/jupyterhub-deeplearning:v2024-09-12",
        },
        "gpu_counts": [1, 2, 4, 8],
        "node_info": node_info["blackpearl"],
        "server_type": "gpu-only" # can be: "cpu-gpu", "cpu-only", and "gpu-only"
      }

      # options for users to select from
      c.KubeSpawner.options_form = dynamic_options_form_withconfig(CONFIG)

      # pre-spawn hook to set resource limits based on user selections
      c.KubeSpawner.pre_spawn_hook = lambda spawner: set_spawner_resources(spawner, CONFIG)
